# Plan D√©taill√© 08 : Plan d'Action pour l'Optimisation IA (Phase 2)

**Statut** : En Planification
**Date** : 09/12/2025
**Objectif** : Am√©liorer le ratio **Temps de R√©ponse / Complexit√©** du service IA.

Actuellement, le syst√®me fonctionne ("Lite Mode") mais subit des latences importantes dues au mat√©riel local (CPU inference). Ce plan vise √† fluidifier l'exp√©rience sans sacrifier la qualit√© musicale.

---

## 1. Axe "Prompt Engineering & Structure" (Logiciel)

### 1.1 Grammaires Ollama (BNF)
Ollama supporte d√©sormais les grammaires GBNF. Plut√¥t que de dire "Donne moi du JSON", nous pouvons forcer le moteur √† ne g√©n√©rer **QUE** des tokens conformes au sch√©ma JSON.
*   **Gain** : Fini les erreurs de parsing ("```json"). Fini le "bavardage" (chatiness).
*   **Action** : Cr√©er un fichier `.gbnf` pour la structure `MelodySequence` et le passer √† l'API Ollama.

### 1.2 "Fill-in-the-Middle" (FIM)
Au lieu de g√©n√©rer une m√©lodie "from scratch", on peut pr√©-remplir une structure rythmique simple et demander √† l'IA de "colorier" les hauteurs de notes.
*   **Gain** : Moins de charge cognitive pour le mod√®le, r√©sultats plus rapides.

### 1.3 Few-Shot Learning "Compress√©"
Injecter 2 ou 3 exemples de m√©lodies JSON *tr√®s courtes* (1 mesure) directement dans le prompt syst√®me.
*   **Gain** : Le mod√®le "imite" la structure imm√©diatement, r√©duisant le temps de convergence.

---

## 2. Axe Technique (Infrastructure Local)

### 2.1 Quantization Agressive (Model Tuning)
Le mod√®le `mistral` standard (7B) est lourd (~4GB VRAM).
*   **Action** : Tester des variantes quantifi√©es plus agressives si le CPU est le goulot d'√©tranglement :
    *   `q4_k_m` (Standard actuel, bon √©quilibre).
    *   `q2_k` (Tr√®s rapide, mais risque de r√©sultats musicaux "cass√©s").
    *   **Alternative** : Passer sur des mod√®les plus petits sp√©cialis√©s (ex: `TinyLlama` 1.1B ou `Phi-2` 2.7B) fine-tun√©s pour le JSON.

### 2.2 Gestion du "Cold Start" (Keep-Alive)
Ollama d√©charge le mod√®le de la m√©moire apr√®s 5 min d'inactivit√© (par d√©faut).
*   **Probl√®me** : Le premier clic prend 30s (chargement), les suivants 5s.
*   **Action** : Configurer Ollama pour garder le mod√®le en RAM (`keep_alive: -1` ou `60m`) lors de la session de travail.

### 2.3 Param√®tres d'Inf√©rence
*   `num_ctx` : R√©duire strictement √† 1024 ou 512 si on ne g√©n√®re que 2 mesures. Moins de contexte = Moins de RAM = Plus vite.
*   `num_thread` : Ajuster au nombre de coeurs physiques CPU (ne pas surcharger).

---

## 3. Axe UX & M√©thode de Discussion

### 3.1 Streaming (Feedback Temps R√©el)
Actuellement, on attend la fin du JSON (bloquant).
*   **Id√©e** : Parser le JSON "au fil de l'eau" (compliqu√©) ou afficher un texte "Thinking..." qui montre les "Pens√©es" de l'IA si on utilisait une chaine de pens√©e.
*   **Mieux** : G√©n√©ration progressive mesure par mesure (Chain of Requests).
    1.  G√©n√®re Mesure 1 -> Affiche/Joue.
    2.  G√©n√®re Mesure 2 -> Append.

### 3.2 Mode "Chat" (Raffinement)
Au lieu de "One-Shot" (G√©n√©rer -> Remplacer), passer √† un mode conversationnel.
*   **Workflow** :
    1.  User: "Bassline funk" -> IA g√©n√®re V1.
    2.  User: "Plus rapide" -> IA modifie V1 (au lieu de tout refaire).
*   **Impl√©mentation** : Envoyer l'historique JSON pr√©c√©dent dans le contexte (Attention √† la taille du contexte !).

## 4. Strat√©gies Avanc√©es (Architecture V2)

### 4.1 "Cerveau Hybride" (Priorit√© P1 - Court Terme)
**Valid√© par User**. Utiliser Python pour pr√©-calculer les r√®gles strictes et soulager l'IA.
*   **Principe** : Python calcule la "Piscine de Notes" (ex: Gamme C Minor = C, D, Eb...) et les r√®gles de chevauchement.
*   **Impl√©mentation** : Script `MusicTheoryService.py` qui g√©n√®re une liste de contraintes inject√©e dans le Prompt.

### 4.2 "Micro-RAG" (Priorit√© P2 - Moyen Terme)
Pour ~50 pages de th√©orie, une base vectorielle (ChromaDB) est overkill.
*   **M√©thode "Keyword Injection"** : Si le prompt contient "Jazz II-V-I", Python charge le fichier texte `docs/theory/jazz.txt` et le colle dans le System Prompt.
*   **Gain** : Contextualisation forte sans latence d'indexation lourde.

### 4.3 Fine-Tuning "Small Model" (Priorit√© P3 - Long Terme)
Cr√©er un mod√®le sp√©cialiste (ex: `TinyLlama-1.1B`) qui ne fait QUE du JSON musical.
*   **Dataset** : Convertir des datasets MIDI publics (Maestro, Lakh) vers notre format JSON.
*   **Gain** : Vitesse extr√™me (< 1s) et robustesse syntaxique 100%.

## 4. Plan de Mise en ≈íuvre (Priorit√©s)

| Priorit√© | Action | Complexit√© | Gain Est. |
| :--- | :--- | :--- | :--- |
| **P1** | **Ollama Keep-Alive** (Config Docker) | Faible | ‚úÖ Fait |
| **P1** | **R√©duction Context (`num_ctx=2048`)** | Faible | ‚úÖ Fait |
| **P1** | **Switch Mod√®le (`qwen2.5:1.5b`)** | Faible | ‚ùå Obsol√®te (Trop limit√©) |
| **P1.5** | **Upgrade Mod√®le (`qwen2.5:3b`)** | Faible | üîÑ En Cours (Qualit√©) |
| **P1** | **Prompt V2 (R√®gles Musicales + Notation #)** | Moyenne | ‚úÖ Fait |
| **P3** | **Grammaire GBNF** | √âlev√©e (Code) | ‚è∏Ô∏è En Attente (Pas n√©cessaire pour l'instant) |
| **P4** | **UX Streaming / Chat** | √âlev√©e (Frontend) | ‚≠ê‚≠ê (Ressenti) |

**Recommandation Imm√©diate** : Commencer par P1 (Config) et P2 (Tester un petit mod√®le 1B-3B param√®tres).
