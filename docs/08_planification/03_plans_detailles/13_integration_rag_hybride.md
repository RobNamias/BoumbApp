# Plan D√©taill√© 13 : Int√©gration RAG Hybride (Perplexity + Qwen)

**Statut** : üü¢ Termin√©
**Date** : 17/12/2025
**R√©f. Architecture** : `docs/09_specialisation_ia_avancee/06_architecture_rag_hybride.md`

## 1. Vision & Objectifs
Cr√©er un pipeline de g√©n√©ration musicale "intelligent" qui :
1.  **Cherche** la th√©orie musicale via Perplexity API ("Teacher").
2.  **Apprend** en stockant ce savoir dans un *Micro-RAG* (Fichiers Markdown).
3.  **Compose** en utilisant un SLM Local (Qwen 1.5B/3B) nourri par ce contexte ("Student").

> **Analogie** : On ne demande pas √† l'√©tudiant (Qwen) d'inventer la th√©orie. On lui donne le manuel (√©crit par Perplexity) et on lui demande de faire l'exercice.

## 2. D√©coupage Technique

## 2. D√©coupage Technique : "Separate Build & Run"

Cette architecture s√©pare la **Constitution du Savoir** de son **Utilisation**, pour des raisons de co√ªt et de s√©curit√©.

### Phase A : "Le Biblioth√©caire" (Offline / CLI)
Un script Python autonome (`librarian.py`), ex√©cutable hors du conteneur principal si besoin.
*   **Mission** : Cr√©er le Micro-RAG.
*   **Input** : Une liste de "Sujets" (ex: `topics.yaml` ou argument CLI).
*   **Outil** : Cl√© API Perplexity.
*   **Output** : √âcrit des fichiers Markdown dans `data/rag_knowledge/`.
*   **Avantage** :
    *   Pas besoin de la Cl√© API en Prod/Runtime.
    *   On peut "pr√©chauffer" le RAG avec 50 styles musicaux d'un coup.
    *   Ex√©cution possible en local, sur un autre conteneur, ou via une t√¢che CRON.

### Phase B : "L'Orchestrateur" (Runtime / App)
Le service `ai_service` (Docker) int√©gr√© √† l'app.
*   **Mission** : Composer de la musique.
*   **Acc√®s RAG** : **Lecture Seule**. Il ne fait *jamais* d'appel Perplexity (sauf fonctionnalit√© "Live Research" explicite).
*   **Flux** :
    1.  User demande "Funk".
    2.  App cherche `funk.md`.
    3.  Si trouv√© -> Utilise Qwen.
    4.  Si pas trouv√© -> Erreur "Je ne connais pas ce style" (ou fallback mode g√©n√©rique).

### Phase C : "L'Interface" (Frontend React)
*   **Composant `AIPrompt`** :
    *   Liste d√©roulante des "Styles Connus" (bas√©e sur les fichiers MD pr√©sents).
    *   Champ libre (pour raffiner).
    *   Feedback : "Utilisation du contexte : Bass Funk (Source: Perplexity)".

## 3. Plan d'Action (Step-by-Step)

### √âtape 1 : Infrastructure & POC (Backend)
-#### [DONE] [bulk_librarian.py](file:///d:\Dev\Perso\Web\BOUMBAPP\Application\BoumbApp\ai_service\bulk_librarian.py)
- [x] Auto-generate directories based on category (`styles/`, `theory/`).
- [x] Integrate Perplexity for content fetching.
- [x] Implement Inbox/Archive workflow.
- [x] Auto-populate `glossaire.md`.
- [x] Auto-update `index.md` files.

#### [DONE] [rag_loader.py](file:///d:\Dev\Perso\Web\BOUMBAPP\Application\BoumbApp\ai_service\rag_pipeline\rag_loader.py)
- [x] Implement recursive search.
- [x] Add Fuzzy Keyword Scoring for better matching.
- [x] Ignored non-content files (index, glossary) for search.

## Verification Plan
- [x] Run `librarian.py --category styles/techno "Acid Line"` -> Check file creation.
- [x] Run `bulk_librarian.py` with `inbox.txt` -> Check archive move and glossary update.
- [x] Test `rag_loader` with fuzzy query "Acid Melody" -> Verify it finds the file.nd
- [x] Endpoint `POST /generate` mis √† jour pour accepter `{"topic": "...", "use_rag": true}`.
- [x] Int√©gration du pipeline dans FastAPI.

### √âtape 3 : Frontend Sync
- [x] Cr√©ation du composant `AIPrompt` dans `SynthPanel`.
- [x] Connexion au backend.

## 4. Questions Ouvertes & Risques
1.  **Latence** : Perplexity prend 2-5s. Ollama prend 2-10s. L'utilisateur attendra 15s. Est-ce acceptable ? (UI "Loader" captivante requise).
2.  **Co√ªt Perplexity** : On commence avec la cl√© de l'utilisateur. Il faudra g√©rer le cas "Quota d√©pass√©".
3.  **Format de sortie** : Qwen 1.5B est petit. Il faut √™tre tr√®s strict sur le format JSON demand√© (GBNF grammar ?).

## 5. Prochaine Action
Lancer l'√âtape 1 : Valider qu'on sait parler √† Perplexity depuis le Python.
